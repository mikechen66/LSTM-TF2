{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:03:13.528704Z",
     "start_time": "2020-10-01T07:03:11.616714Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:03:30.971832Z",
     "start_time": "2020-10-01T07:03:30.919332Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:17.819473Z",
     "start_time": "2020-10-01T07:04:16.967326Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPU\n"
     ]
    }
   ],
   "source": [
    "######## GPU CONFIGS FOR RTX 2070 ###############\n",
    "## Please ignore if not training on GPU       ##\n",
    "## this is important for running CuDNN on GPU ##\n",
    "\n",
    "# check if GPU can be seen by TF\n",
    "tf.config.list_physical_devices('GPU')\n",
    "# enable this line if you wish to see whether GPU/CPU executes\n",
    "# each command\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only use the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n",
    "  except RuntimeError as e:\n",
    "    # Visible devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "###############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:34.432270Z",
     "start_time": "2020-10-01T07:04:34.425717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abstract_reasoning, accentdb, aeslc, aflw2k3d, ag_news_subset, ai2_arc, ai2_arc_with_ir, amazon_us_reviews, anli, arc, bair_robot_pushing_small, bccd, beans, big_patent, bigearthnet, billsum, binarized_mnist, binary_alpha_digits, blimp, bool_q, c4, caltech101, caltech_birds2010, caltech_birds2011, cars196, cassava, cats_vs_dogs, celeb_a, celeb_a_hq, cfq, chexpert, cifar10, cifar100, cifar10_1, cifar10_corrupted, citrus_leaves, cityscapes, civil_comments, clevr, clic, clinc_oos, cmaterdb, cnn_dailymail, coco, coco_captions, coil100, colorectal_histology, colorectal_histology_large, common_voice, coqa, cos_e, cosmos_qa, covid19sum, crema_d, curated_breast_imaging_ddsm, cycle_gan, deep_weeds, definite_pronoun_resolution, dementiabank, diabetic_retinopathy_detection, div2k, dmlab, downsampled_imagenet, dsprites, dtd, duke_ultrasound, emnist, eraser_multi_rc, esnli, eurosat, fashion_mnist, flic, flores, food101, forest_fires, fuss, gap, geirhos_conflict_stimuli, genomics_ood, german_credit_numeric, gigaword, glue, goemotions, gpt3, groove, gtzan, gtzan_music_speech, hellaswag, higgs, horses_or_humans, i_naturalist2017, imagenet2012, imagenet2012_corrupted, imagenet2012_real, imagenet2012_subset, imagenet_a, imagenet_r, imagenet_resized, imagenet_v2, imagenette, imagewang, imdb_reviews, irc_disentanglement, iris, kitti, kmnist, lfw, librispeech, librispeech_lm, libritts, ljspeech, lm1b, lost_and_found, lsun, malaria, math_dataset, mctaco, mnist, mnist_corrupted, movie_lens, movie_rationales, movielens, moving_mnist, multi_news, multi_nli, multi_nli_mismatch, natural_questions, natural_questions_open, newsroom, nsynth, nyu_depth_v2, omniglot, open_images_challenge2019_detection, open_images_v4, openbookqa, opinion_abstracts, opinosis, opus, oxford_flowers102, oxford_iiit_pet, para_crawl, patch_camelyon, paws_wiki, paws_x_wiki, pet_finder, pg19, places365_small, plant_leaves, plant_village, plantae_k, qa4mre, qasc, quickdraw_bitmap, radon, reddit, reddit_disentanglement, reddit_tifu, resisc45, robonet, rock_paper_scissors, rock_you, salient_span_wikipedia, samsum, savee, scan, scene_parse150, scicite, scientific_papers, sentiment140, shapes3d, smallnorb, snli, so2sat, speech_commands, spoken_digit, squad, stanford_dogs, stanford_online_products, starcraft_video, stl10, sun397, super_glue, svhn_cropped, ted_hrlr_translate, ted_multi_translate, tedlium, tf_flowers, the300w_lp, tiny_shakespeare, titanic, trec, trivia_qa, tydi_qa, uc_merced, ucf101, vctk, vgg_face2, visual_domain_decathlon, voc, voxceleb, voxforge, waymo_open_dataset, web_questions, wider_face, wiki40b, wikihow, wikipedia, wikipedia_toxicity_subtypes, wine_quality, winogrande, wmt14_translate, wmt15_translate, wmt16_translate, wmt17_translate, wmt18_translate, wmt19_translate, wmt_t2t_translate, wmt_translate, wordnet, xnli, xquad, xsum, yelp_polarity_reviews, yes_no'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see available tfds data sets\n",
    "\", \".join(tfds.list_builders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:37.653833Z",
     "start_time": "2020-10-01T07:04:37.554098Z"
    }
   },
   "outputs": [],
   "source": [
    "# using TFDS dataset\n",
    "# note: as_supervised converts dicts to tuples\n",
    "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", \n",
    "                                with_info=True, as_supervised=True)\n",
    "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:40.136066Z",
     "start_time": "2020-10-01T07:04:40.130384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfds.core.DatasetInfo(\n",
      "    name='imdb_reviews',\n",
      "    version=1.0.0,\n",
      "    description='Large Movie Review Dataset.\n",
      "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
      "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
      "    features=FeaturesDict({\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
      "        'text': Text(shape=(), dtype=tf.string),\n",
      "    }),\n",
      "    total_num_examples=100000,\n",
      "    splits={\n",
      "        'test': 25000,\n",
      "        'train': 25000,\n",
      "        'unsupervised': 50000,\n",
      "    },\n",
      "    supervised_keys=('text', 'label'),\n",
      "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
      "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
      "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
      "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
      "      month     = {June},\n",
      "      year      = {2011},\n",
      "      address   = {Portland, Oregon, USA},\n",
      "      publisher = {Association for Computational Linguistics},\n",
      "      pages     = {142--150},\n",
      "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
      "    }\"\"\",\n",
      "    redistribution_info=,\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:43.390551Z",
     "start_time": "2020-10-01T07:04:43.326285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string) \n",
      " tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for example, label in imdb_train.take(1):\n",
    "    print(example, '\\n', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:04:54.364677Z",
     "start_time": "2020-10-01T07:04:47.490172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the default tokenizer settings\n",
    "tokenizer = tfds.deprecated.text.Tokenizer()\n",
    "\n",
    "vocabulary_set = set()\n",
    "MAX_TOKENS = 0\n",
    "\n",
    "for example, label in imdb_train:\n",
    "  some_tokens = tokenizer.tokenize(example.numpy())\n",
    "  if MAX_TOKENS < len(some_tokens):\n",
    "        MAX_TOKENS = len(some_tokens)\n",
    "  vocabulary_set.update(some_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:55.711915Z",
     "start_time": "2020-10-01T07:05:55.538523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93931 2525\n"
     ]
    }
   ],
   "source": [
    "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, \n",
    "                                                   tokenizer=tokenizer)\n",
    "vocab_size = imdb_encoder.vocab_size\n",
    "\n",
    "print(vocab_size, MAX_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:57.667656Z",
     "start_time": "2020-10-01T07:05:57.624948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "# Lets verify tokenization and encoding works\n",
    "for example, label in imdb_train.take(1):\n",
    "    print(example)\n",
    "    encoded = imdb_encoder.encode(example.numpy())\n",
    "    print(imdb_encoder.decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:05:59.963100Z",
     "start_time": "2020-10-01T07:05:59.954134Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformation fucntions to be used with the dataset\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "def encode_pad_transform(sample):\n",
    "    encoded = imdb_encoder.encode(sample.numpy())\n",
    "    pad = sequence.pad_sequences([encoded], padding='post', \n",
    "                                 maxlen=150)\n",
    "    return np.array(pad[0], dtype=np.int64)  \n",
    "\n",
    "\n",
    "def encode_tf_fn(sample, label):\n",
    "    encoded = tf.py_function(encode_pad_transform, \n",
    "                                       inp=[sample], \n",
    "                                       Tout=(tf.int64))\n",
    "    encoded.set_shape([None])\n",
    "    label.set_shape([])\n",
    "    return encoded, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:00.776090Z",
     "start_time": "2020-10-01T07:06:00.703019Z"
    }
   },
   "outputs": [],
   "source": [
    "# test the transformation on a small subset\n",
    "subset = imdb_train.take(10)\n",
    "tst = subset.map(encode_tf_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:01.845914Z",
     "start_time": "2020-10-01T07:06:01.641109Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[13581 47819 62328 56061 52535 30401 66588 43369 89223 35512  5387 16151\n",
      " 33449 55840 25146 90121 48496 49184 91115 41694 24468 67269   992 86596\n",
      "   289 89223 81016 32667 16740  5387 11563 63550 81016 41694 19153 39306\n",
      " 58448 61911   992 30401 10785 84483 84513 13581 30401 47380 62328 37011\n",
      "  8890 82236 80035 76231 90746  2305 33836 75675 25701 91413 84878 40983\n",
      " 92023 70812 25701 52378 81016 15444 71409 69100 56902 58046 73787  3325\n",
      " 28283 10365 18109 47234 11103 80501 41434 55840 47819 23811 67269 52249\n",
      " 33836 47236 18835  5387 52249 30401 79063 47819  1035 62319 92273 84521\n",
      " 68463 33260  2977 53231 79063 80750 91115 81296 10884   992 11633 71708\n",
      " 10785 10884 33449 55840 10785 54985 34630 33260 39306 36917 48396 48547\n",
      " 81137     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0], shape=(150,), dtype=int64) tf.Tensor(0, shape=(), dtype=int64)\n",
      "This was an absolutely terrible movie Don t be lured in by Christopher Walken or Michael Ironside Both are great actors but this must simply be their worst role in history Even their great acting could not redeem this movie s ridiculous storyline This movie is an early nineties US propaganda piece The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions Maria Conchita Alonso appeared phony and her pseudo love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning I am disappointed that there are movies like this ruining actor s like Christopher Walken s good name I could barely sit through it\n"
     ]
    }
   ],
   "source": [
    "for review, label in tst.take(1):\n",
    "    print(review, label)\n",
    "    print(imdb_encoder.decode(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:03.379137Z",
     "start_time": "2020-10-01T07:06:03.359510Z"
    }
   },
   "outputs": [],
   "source": [
    "#now tokenize/encode/pad all training\n",
    "# and testing data\n",
    "encoded_train = imdb_train.map(encode_tf_fn,\n",
    "                               num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "encoded_test = imdb_test.map(encode_tf_fn,\n",
    "                             num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:19.472832Z",
     "start_time": "2020-10-01T07:06:19.467992Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 64\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:21.557245Z",
     "start_time": "2020-10-01T07:06:21.550589Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:26.612154Z",
     "start_time": "2020-10-01T07:06:25.825486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (100, None, 64)           6011584   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (100, 64)                 33024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (100, 1)                  65        \n",
      "=================================================================\n",
      "Total params: 6,044,673\n",
      "Trainable params: 6,044,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model_lstm(vocab_size = vocab_size,\n",
    "                         embedding_dim=embedding_dim,\n",
    "                         rnn_units=rnn_units,\n",
    "                         batch_size=BATCH_SIZE)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:31.817335Z",
     "start_time": "2020-10-01T07:06:31.799369Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:06:37.541747Z",
     "start_time": "2020-10-01T07:06:37.536302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prefetch for performance\n",
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:10:27.683509Z",
     "start_time": "2020-10-01T07:06:40.292699Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 19s 75ms/step - loss: 0.4200 - accuracy: 0.8013 - precision: 0.7810 - recall: 0.8374\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.1716 - accuracy: 0.9388 - precision: 0.9391 - recall: 0.9386\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 20s 82ms/step - loss: 0.1007 - accuracy: 0.9669 - precision: 0.9691 - recall: 0.9646\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.0783 - accuracy: 0.9737 - precision: 0.9738 - recall: 0.9736\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.0437 - accuracy: 0.9852 - precision: 0.9861 - recall: 0.9843\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 21s 83ms/step - loss: 0.0276 - accuracy: 0.9918 - precision: 0.9926 - recall: 0.9910\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 21s 85ms/step - loss: 0.0382 - accuracy: 0.9877 - precision: 0.9881 - recall: 0.9873\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0247 - accuracy: 0.9922 - precision: 0.9919 - recall: 0.9924\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 21s 84ms/step - loss: 0.0228 - accuracy: 0.9929 - precision: 0.9925 - recall: 0.9933\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 21s 82ms/step - loss: 0.0216 - accuracy: 0.9936 - precision: 0.9940 - recall: 0.9931\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f20c8065a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:10:55.965511Z",
     "start_time": "2020-10-01T07:10:34.119008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 20s 78ms/step - loss: 0.7636 - accuracy: 0.8349 - precision: 0.8458 - recall: 0.8191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7636314034461975, 0.8349199891090393, 0.845848798751831, 0.8191199898719788]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:23.892847Z",
     "start_time": "2020-10-01T07:11:23.887880Z"
    }
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = imdb_encoder.vocab_size # len(chars)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "\n",
    "#batch size\n",
    "BATCH_SIZE=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:25.201887Z",
     "start_time": "2020-10-01T07:11:25.192055Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout=0.2\n",
    "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, return_sequences=True, dropout=0.25)),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units, dropout=0.25)),\n",
    "    tf.keras.layers.Dropout(dropout),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:29.441086Z",
     "start_time": "2020-10-01T07:11:26.487139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (50, None, 128)           12023168  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (50, None, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (50, None, 128)           98816     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (50, None, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (50, 128)                 98816     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (50, 128)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (50, 1)                   129       \n",
      "=================================================================\n",
      "Total params: 12,220,929\n",
      "Trainable params: 12,220,929\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstm = build_model_bilstm(vocab_size = vocab_size,\n",
    "                            embedding_dim=embedding_dim,\n",
    "                            rnn_units=rnn_units,\n",
    "                            batch_size=BATCH_SIZE)\n",
    "bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:29.575709Z",
     "start_time": "2020-10-01T07:11:29.562501Z"
    }
   },
   "outputs": [],
   "source": [
    "bilstm.compile(loss='binary_crossentropy', \n",
    "               optimizer='adam', \n",
    "               metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:11:32.251708Z",
     "start_time": "2020-10-01T07:11:32.245767Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:17:29.418810Z",
     "start_time": "2020-10-01T07:11:34.295046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 [==============================] - 64s 128ms/step - loss: 0.3738 - accuracy: 0.8256 - precision: 0.8204 - recall: 0.8337\n",
      "Epoch 2/5\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.1467 - accuracy: 0.9462 - precision: 0.9467 - recall: 0.9456\n",
      "Epoch 3/5\n",
      "500/500 [==============================] - 65s 131ms/step - loss: 0.0745 - accuracy: 0.9753 - precision: 0.9757 - recall: 0.9749\n",
      "Epoch 4/5\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.0515 - accuracy: 0.9823 - precision: 0.9829 - recall: 0.9817\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 66s 132ms/step - loss: 0.0270 - accuracy: 0.9902 - precision: 0.9905 - recall: 0.9899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f209f26a490>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.fit(encoded_train_batched, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T07:17:50.858013Z",
     "start_time": "2020-10-01T07:17:29.532181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 16s 32ms/step - loss: 0.6841 - accuracy: 0.8360 - precision: 0.8250 - recall: 0.8529\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6841403245925903,\n",
       " 0.8360000252723694,\n",
       " 0.8250271081924438,\n",
       " 0.8528800010681152]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.evaluate(encoded_test.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release GPU memory\n",
    "from numba import cuda\n",
    "\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
